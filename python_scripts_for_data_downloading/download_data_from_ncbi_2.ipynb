{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def get_lists_platforms_and_series():\n",
    "    \n",
    "    rows = []\n",
    "        \n",
    "    with open('geo_samples.tsv','r') as tsvin, open('series_vs_platforms.csv', 'w') as csvout:\n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "        csvout = csv.writer(csvout, delimiter='\\t')\n",
    "        indexes = []\n",
    "        \n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                indexes = [row.index(j) for j in ['Series', 'Taxonomy', 'Platform']]\n",
    "                rows.append(['Series', 'Platforms'])\n",
    "            \n",
    "            if row[indexes[1]].lower() in {\"homo sapiens\", \"mus musculus\", \"rattus norvegicus\"}:\n",
    "                print(i)\n",
    "                ser = row[indexes[0]].split(\";\")\n",
    "                for s in ser: \n",
    "                    r = [s, row[indexes[2]]]\n",
    "                    if r not in rows:\n",
    "                        rows.append(r)\n",
    "                        \n",
    "        print(\"Length rows: \", len(rows))\n",
    "        csvout.writerows(rows)\n",
    "                        \n",
    "\n",
    "get_lists_platforms_and_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e', 't'], ['r', 'yy']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "arr = []\n",
    "with open('table.csv', 'w') as csvout:\n",
    "    csvout = csv.writer(csvout, delimiter='\\t')\n",
    "    arr.append(['e', 't'])\n",
    "    arr.append(['r', 'yy'])\n",
    "    w = ['e', 't']\n",
    "    if w not in arr: \n",
    "        print(\"not\")\n",
    "        arr.append(w)\n",
    "    # for a in arr:\n",
    "    #     for key, value in a.items():\n",
    "    #         print(key, \" \", value)\n",
    "    print(arr) \n",
    "    csvout.writerows(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def get_paths(file_name):\n",
    "    \n",
    "    paths = {}\n",
    "    \n",
    "    with open(file_name,'r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin, delimiter=',')\n",
    "        indexes = []\n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                indexes = [row.index(j) for j in ['key', 'path']]\n",
    "                \n",
    "            paths[row[indexes[0]]] = row[indexes[1]]\n",
    "            \n",
    "    return paths\n",
    "\n",
    "\n",
    "\n",
    "def get_platforms_quantity_for_each_serie():\n",
    "    import collections\n",
    "    \n",
    "    series = []\n",
    "    \n",
    "    with open('series_vs_platforms.csv','r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "        indexes = []\n",
    "        \n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                indexes = [row.index(j) for j in ['Series', 'Platforms']]\n",
    "            \n",
    "            series.append(row[indexes[0]])\n",
    "    \n",
    "    duplicated_series = \\\n",
    "            [item for item, count in collections.Counter(series).items() if count > 1]\n",
    "    \n",
    "    quantity = {}\n",
    "    count_not_in_s = 0\n",
    "    \n",
    "    for s in series:\n",
    "        if s not in quantity:\n",
    "            quantity[s] = 'duplicate' if s in duplicated_series else 'single'\n",
    "    \n",
    "    return quantity\n",
    "\n",
    "\n",
    "\n",
    "def get_full_file_with_urls_series_and_platforms(series_paths, platforms_paths, quantity):\n",
    "    rows = []\n",
    "        \n",
    "    with open('series_vs_platforms.csv','r') as tsvin, \\\n",
    "                        open('urls_series_and_platforms_full_list.csv', 'w') as csvout:\n",
    "            \n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "        csvout = csv.writer(csvout, delimiter='\\t')\n",
    "        indexes = []\n",
    "        \n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                indexes = [row.index(j) for j in ['Series', 'Platforms']]\n",
    "                rows.append(['Series', 'Series_url', 'Platforms', 'Platforms_url'])\n",
    "            \n",
    "            if row[indexes[0]] in series_paths:\n",
    "                serie_url = 'https://ftp.ncbi.nlm.nih.gov/geo/series/' + \\\n",
    "                                        series_paths[row[indexes[0]]]\n",
    "\n",
    "                if quantity[row[indexes[0]]] == 'duplicate':\n",
    "                    serie = row[indexes[0]] + '-' + row[indexes[1]] + '_series_matrix.txt.gz'\n",
    "                else:\n",
    "                    serie = row[indexes[0]] + '_series_matrix.txt.gz'\n",
    "\n",
    "                serie_url = serie_url + serie\n",
    "                \n",
    "                if row[indexes[1]] in platforms_paths:\n",
    "                    platform_url = 'https://ftp.ncbi.nlm.nih.gov/geo/platforms/' + \\\n",
    "                                            platforms_paths[row[indexes[1]]] + row[indexes[1]] + \\\n",
    "                                            '.annot.gz'\n",
    "                    rows.append([row[indexes[0]], serie_url, row[indexes[1]], platform_url])   \n",
    "            \n",
    "        csvout.writerows(rows)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_paths = get_paths('list_series.csv')\n",
    "platforms_paths = get_paths('list_platforms.csv')\n",
    "quantity = get_platforms_quantity_for_each_serie()\n",
    "print('Strart fill url file')\n",
    "get_full_file_with_urls_series_and_platforms(series_paths, platforms_paths, quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fill platforms\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def get_full_list_urls_separate(file_name, kind_data):\n",
    "    rows = []\n",
    "        \n",
    "    with open('urls_series_and_platforms_full_list.csv','r') as tsvin, \\\n",
    "                        open(file_name, 'w') as csvout:\n",
    "            \n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "        csvout = csv.writer(csvout, delimiter='\\t')\n",
    "        indexes = []\n",
    "        \n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                indexes = [row.index(j) for j in \\\n",
    "                           ['Series', 'Series_url', 'Platforms', 'Platforms_url']]\n",
    "                if kind_data == 'series':\n",
    "                    rows.append(['Series', 'Series_url'])\n",
    "                else:\n",
    "                    rows.append(['Platforms', 'Platforms_url'])\n",
    "            else:\n",
    "                r = []\n",
    "                if kind_data == 'series':\n",
    "                    r = [row[indexes[0]], row[indexes[1]]]\n",
    "                else:\n",
    "                    r = [row[indexes[2]], row[indexes[3]]]\n",
    "                rows.append(r)     \n",
    "\n",
    "        csvout.writerows(rows)\n",
    "\n",
    "\n",
    "get_full_list_urls_separate('full_list_urls_series.csv', 'series')\n",
    "print('Start fill platforms')\n",
    "get_full_list_urls_separate('full_list_urls_platforms.csv', 'platforms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def get_short_list_urls_separate(file_name_for_reading, file_name_for_writing,\\\n",
    "                                file_name_with_current_data, kind_data):\n",
    "    rows = []\n",
    "    current_data = format_file(file_name_with_current_data)\n",
    "        \n",
    "    with open(file_name_for_reading,'r') as tsvin, \\\n",
    "                        open(file_name_for_writing, 'w') as csvout:\n",
    "            \n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "        csvout = csv.writer(csvout, delimiter='\\t')\n",
    "        indexes = []\n",
    "        \n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                if kind_data == 'series':\n",
    "                    indexes = [row.index(j) for j in ['Series', 'Series_url']]\n",
    "                    rows.append(['Series', 'Series_url'])\n",
    "                else:\n",
    "                    indexes = [row.index(j) for j in ['Platforms', 'Platforms_url']]\n",
    "                    rows.append(['Platforms', 'Platforms_url'])\n",
    "            else: \n",
    "                dif_string = 'https://ftp.ncbi.nlm.nih.gov/geo/' + kind_data + '/'\n",
    "                path = row[indexes[1]].replace(dif_string, '')\n",
    "                               \n",
    "                if path not in current_data:\n",
    "                    rows.append([row[indexes[0]], row[indexes[1]]])     \n",
    "\n",
    "        csvout.writerows(rows)\n",
    "\n",
    "\n",
    "\n",
    "def format_file(file_name):\n",
    "    lines = [line.rstrip('\\n') for line in open(file_name)]\n",
    "    for i in range(0,len(lines),1):\n",
    "        lines[i] = lines[i].replace('./', '')\n",
    "    return lines\n",
    "\n",
    "\n",
    "get_short_list_urls_separate('full_list_urls_series.csv', \\\n",
    "                'short_list_urls_series.csv', 'current_series.txt', 'series')\n",
    "                               \n",
    "# print('Start fill platforms')\n",
    "                               \n",
    "# get_short_list_urls_separate('full_list_urls_platforms.csv', \\\n",
    "#                 'short_list_urls_platforms.csv', 'current_platforms.txt', 'platforms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import csv\n",
    "\n",
    "\n",
    "def download_data(file_name, kind_data): \n",
    "    \n",
    "    with open(file_name,'r') as tsvin:\n",
    "        tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "        indexes = []\n",
    "        \n",
    "        for (i, row) in enumerate(tsvin):\n",
    "            if i == 0:\n",
    "                if kind_data == 'series':\n",
    "                    indexes = [row.index(j) for j in ['Series', 'Series_url']]\n",
    "                else:\n",
    "                    indexes = [row.index(j) for j in ['Platforms', 'Platforms_url']]\n",
    "            else:    \n",
    "                try:\n",
    "                    dif_string = 'https://ftp.ncbi.nlm.nih.gov/'\n",
    "                    path = row[indexes[1]].replace(dif_string, '')\n",
    "                    path = path[:(path.rindex('/')+1)]\n",
    "                    directory = \"../\" + path\n",
    "\n",
    "                    subprocess.call([\"mkdir\", \"-p\", directory])\n",
    "                    subprocess.call([\"wget\", \"-c\", \"-P\", directory, row[indexes[1]]])\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print('current', kind_data, ': ', row[indexes[0]], 'url: ', \\\n",
    "                                          row[indexes[1]], ' with i: ', i)\n",
    "\n",
    "        \n",
    "        \n",
    "download_data(\"short_list_urls_platforms.csv\", 'platforms')\n",
    "#download_data('short_list_urls_series.csv', 'series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42474"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line.rstrip('\\n') for line in open(\"current_series.txt\")]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geo/platforms/GPLnnn/GPL570/annot/'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"https://ftp.ncbi.nlm.nih.gov/geo/platforms/GPLnnn/GPL570/annot/GPL570.annot.gz\"\n",
    "dif_string = 'https://ftp.ncbi.nlm.nih.gov/'\n",
    "path = s.replace(dif_string, '')\n",
    "path[:(path.rindex('/')+1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
